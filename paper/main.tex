\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath} 



\title{Neural SDE: phase trajectories of SDE in the action}

\author{ Papay Ivan\\
	\texttt{papai.id@phystech.edu} \\
	%% examples of more authors
	\And
	Vladimirov Eduard \\
 \texttt{vladimirov.ea@phystech.edu} \\
 \And
	Strijov Vadim \\
 \texttt{strijov@phystech.edu} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{2024 год}

%%\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
%\hypersetup{
%pdftitle={A template for the arxiv style},
%pdfsubject={q-bio.NC, q-bio.QM},
%pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
%pdfkeywords={First keyword, Second keyword, More},
%}
\usepackage{algorithm}
\usepackage{algpseudocode}
\begin{document}

\maketitle

\begin{Abstract}
    Данная статья предлагает развить математический аппарат, на котором строится модель Neural SDE. В ней рассмотрено, как вычисление фазовых траекторий стохастических дифференциальных уравнений обеспечивает качественный прогноз аномалий во временном ряду. Таким образом, это предоставит как возможность эффективнее бороться с шумами, так и, в частности, полезный инструмент для упреждения появления так называемых `черных лебедей` - резких перемен поведения временных рядов, как случайных процессов. Подобного рода аномалии способны нарушить корректную работу Neural SDE в виду высокой корреляции элементов анализируемой выборки между собой.
\end{Abstract}


\keywords{SDE \and Stratonovich integral \and Breakdown Points \and More}

\section{Введение}

   \par Сбор данных и подготовка их к последующей обработке являются одной из важнейших задач машинного обучения. Не всегда исследователь может гарантировать их целостность и корректность, ведь для тренировки модели чаще всего требуются выборки из тысяч, а то и десятков тысяч элементов --- не удивительно, что в данных допускается наличие шума, влияющего на работу обученной модели. Эта задача остаётся актуальной и для временных рядов. Нужно, имея данные для начала временного ряда, проверить: возможно ли предсказать его на некотором горизонте так, что это предсказание будет точным, и не сломает ли это природу текущего временного ряда в вероятностном смысле?
   \par В данной работе предполагается, что природа данных стохастическая. Входная выборка представляет из себя данные о некоем дискретном случайном процессе. От дискретного процесса корректно будет перейти к непрерывному в силу того факта, что он порождается сигма-алгеброй из конечномерных распределений, которые реально апроксимировать с помощью данных, предоставленных для обучения модели.
   \par Под данными имеются в виду значения n-мерного случайного процесса в фиксированном конечном множестве точек, которые необходимо будет сжать посредством CCA + CCM~(Convergent-Cross Mapping)[6]. В таком случае в качестве варианта апроксимации полученного одномерного временного ряда предлагается взять метод приближения обыкновенными дифференциальными уравнениями, от которых затем уже перейти к стохастическим.
   \par Сама идея использования обыкновенных дифференциальных уравнений~(`ОДУ`) не нова[1][8]. Так, примерно с 2017-го года она была использована[2] для создания и теоретического обоснования корректности работы модели Neural ODE. Тем не менее, такой метод был всё ещё слаб в робастном смысле[2]: был уязвим к состязательным атакам. Модель Neural SDE[2-4] уже строилась на использовании стохастических дифференциальных уравнений~(`СДУ`) и была в этом плане эффективнее своего предшественника.
   \par Главной целью данного исследования является построение decision-rejection~(принятие-отрицание) критерия корректности той или иной гипотезы о вероятностном распределении входных данных, как некоторого непрерывного случайного процесса. Для проверки фрагмента временного ряда на наличие аномалий достаточно применить этот критерий для проверки гипотезы о тождественности распределений его и всего остального ряда --- разумно будет заключить, что в ряду происходят аномалии, если природа данных в стохастическом смысле резко поменялась.
   \par Таким образом, полагая, что временной ряд порождается определенными конечномерными распределениями, мы сможем приблизить его с помощью стохастических дифференциальных уравнений. То же применимо и к анализируемому диапазону, который требуется проверить на наличие аномалий. Если фазовые траектории полученных дифференциальных уравнений различаются, то есть происходит резкое их возмущение, то очевидно, что в ряду произошла аномалия. Такого рода аномалии отсюда и далее мы будем называть точками разладки.
   \par В прошлых работах, связанных с Neural SDE[2-4], СДУ использовались только для построения доверительных интервалов для элементов временного ряда. Этот подход в статье предлагается развить посредством использования фазовых траекторий полученных СДУ. Таким образом, можно будет проверять большие массивы данных на корреляцию между собой. В работе рассматривается задача --- найти точки разладки для фиксированного временного ряда. То есть для каждой из точек временного ряда проверяется decision-rejection критерий о согласованности поведения ряда в этой конкретной точке и всего остального ряда.
   \par Будет уместным сказать, что ход решения задачи будет схож с методами генеративных моделей[12], а не дискриминативными[9][11]. Требуется предъявить читателю не столько новый способ отделить точки разладки, сколько метод оценки вероятности того, что та или иная точка таковой является. Для корректного решения проблемы следует взять лучшее из обоих подходов аналогично тому, как некоторые исследователи уже делали раньше[10].
   \par Ниже детально разобрано, как по СДУ, построенным по временным рядам, получить фазовые траектории путём зануления диффузии СДУ и его семплирования, как случайной величины. В качестве decision-rejection критерия используется сравнение полученных траекторий в плане их главных компонент.

\section{Связанные работы}
  \subsection{Точки разладки}
    \par Следует тщательно разобраться в том, что означает такое понятие, как чёрный лебедь, заявленное в аннотации статьи. Пускай имеется некоторый временной ряд. Стохастическая природа его известна, но в определенный момент она меняется. Этот момент называется моментом разладки.


    \begin{figure}[htp]
    \centering
    \includegraphics[width=7cm]{local_raz (1).jpg}
    \includegraphics[width=7cm]{global_razlad (1).jpg}
    \caption{примеры разладок}
    \label{fig:galaxy}
    \end{figure}

    \par Точка разладки является моментом локальной разладки, если при игнорировании её природа всего временного ряда в стохастическом плане останется однородной.
    
    \par Глобальной же, если по достижению оной всё поведение временного ряда резко меняется.  
    
  \subsection{Neural ODE}
    \par Изначально Neural ODE был разработан как альтернатива методу остаточных нейронных сетей, состоящих из последовательности скрытых слоёв, значения на каждом из которых подчинялись следующей формуле:
    \begin{equation} h_{k+1} = h_k + f(h_k, w_k)    \end{equation}
    \par --- где $h_k$ - вход k-го слоя для $k \in [1, K]$, $K$-число слоёв и f($h_k$, $w_k$) - нелинейная функция, параметризованная по $w_k$ - динамический параметр, задающийся непосредственно перед началом обучения модели.
    \par Было предложено[5] представление (1) в виде:
    \begin{equation} h_{t} = h_s + \int_s^t f(h_l, l; w) dl,    \end{equation}
    \par --- вычисление такого дифференциального уравнения является задачей для Neural ODE. В данной работе в качестве числа слоев берется число элементов во временном ряду.

    \begin{algorithm}
     \caption{Neural ODE-solver}\label{alg:cap}
     \begin{algorithmic}
    \Require динамические параметры $w$, начальное/конечное время $t_0,t_1$, конечное значение $z(t_1)$, градиент функции потерь в конечной точке $\frac{\delta L}{\delta z(t_1)}$
    \State $s_0 = [z(t_1), \frac{\delta L}{\delta z(t_1)}, 0_{[w]}]$ \Comment{Начальное состояние}
    \State $[z(t_0), \frac{\delta L}{\delta z(t_1)}, \frac{\delta L}{w}] = ODESolve(s_0, [f(z(t), t, w), -a(t)^T \frac{\delta f}{\delta z}, -a(t)^T \frac{\delta f}{\delta w}], t_1, t_0, w)$
    \State $return \frac{\delta L}{\delta z(t_0)}, \frac{\delta L}{w}$ \Comment{Возвращаем градиенты}
\end{algorithmic}
\end{algorithm}
   \par Здесь ODESolve - это черный ящик, возвращающий решение ODE --- в качестве него предлагается использовать метод Рунге-Кутта, а L --- MSE~(mean squared error --- среднее квадратов отклонения элементов выборки от их оценок).

   \subsection{Neural CDE}

   \par В случае анализа временных рядов для выражения(2) было предложено[7] рассматривать не классический интеграл Римана-Стилтьеса, а стохастический интеграл Стратоновича:
   \begin{equation} h_{t} = h_s + \int_s^t f(h_l, l; w) dB_l,    \end{equation}
   \par --- где $B_t=[B_t^1...B_t^K]$ - Винеровский процесс той же размерности, что и $X_t$.
   \par Таким образом получается контролируемое дифференциальное уравнение, подходящее для данных типа Time-Series, особенно когда временные метки ряда не являются регулярными в плане тождественности их расстояний между собой.
   \subsection{Neural SDE}
      \par Обобщением предыдущих двух методов является модель Neural SDE.
      \par Для учёта шума и максимально возможной апроксимации проноза в нашем дифференциальное уравнение следует учесть и недетерменированную компоненту, и диффузию. Получится следующее выражение, являющееся полным стохастическим дифференциалом:
      \begin{equation} dX_t^w = h(t, X_t^w; w) dt + \sigma(X_t^w;w) dB_t     \end{equation}
      \par --- где $B_t=[B_t^1...B_t^K]$ - Винеровский процесс той же размерности, что и $X_t$, а $\sigma(X_t^w;w)$ - его матрица ковариаций в t-й момент времени
      \par Обобщим это выражение для модели ResNet:
      \begin{equation}  dh_t = f(h_t, t, w),    \end{equation}
      \par --- таким образом выражение (1) для (k+1)-го слоя изменится:
      \begin{equation} h_{k+1} = h_k + f(h_k, w_k) +  \sigma(X_k^w;w) B_k,    \end{equation}
      \par --- соответственно алгоритм остаётся тем же, что и для ODE с поправкой на вычисление матрицы ковариации путём семплирования входной выборки.

\section{Постановка задачи}
   
   \par В общем виде для решения поставленной задачи требуется осуществить следующие шаги - применить метод Neural ODE к временному ряду, учесть гауссовский шум и тем самым свести задачу к модели Neural SDE, вычислить фазовые траектории для временного ряда, предварительно свернув исходный многомерный временной ряд к минимально возможному размеру и, наконец, сравнить полученные фазовые траектории временных рядов по поведению с отдельными точками, проверяемыми на элемент разладки. Далее предлагается вкратце их разобрать.
   \par Исходный временной ряд соответственно: \begin{equation}\{X_{t_i}\}_{i=1}^n \end{equation} \par --- где $t_i$ - временные метки, упорядоченные по возрастанию. Предполагается, что траектория ряда подчиняется некоторому СДУ~(см. 2.2)
   \par Решив СДУ с помощью SDE-solver-a мы получим точные значения $f(h_n,w_n)= C_n-\text{const}$(см. (4)) для $\forall n$. Тогда в той же формуле (4) приняв во внимание факт того, что $E B_t=0$, занулим коэффицент диффузии и получим следующее выражение:
   \begin{equation} h_{n+1} = h_n + C_n    \end{equation}
   \par --- это оценки приращений элементов временного ряда. По ним строится матрица Ганкеля, описывающая фазовые траектории процесса, размера n/2 на n/2 вида:
    \begin{equation}
 \begin{bmatrix}
   C_1 & C_2 & \cdots & C_{n/2} \\
   C_2 & C_3 & \cdots & C_{n/2+1} \\
   \vdots  & \vdots  & \ddots & \vdots  \\
   C_{n/2} & C_{n/2+1} & \cdots & C_{n} 
 \end{bmatrix}
\end{equation}
   \par Таким образом аналогично методу SSA получим оценки на значения исходного временного ряда. Среднее каждого из элементов оценивается выборочным средним значений на соответствующей антидиагонали матрицы Ганкеля, а его разброс выборочной дисперсией.
   
   \par В таком случае - хоть и в силу неточности приближения мы неспособны прогнозировать дальнейшее поведение временного ряда, но момент разладки найти возможно. Если сэмплировать получить оценки 95-й и 5-й квантили, зная средние и дисперсии каждого из элементов ряда, то тем самым возможно получить доверительный интервал для его значений. Если в тот или иной момент ряда он резко вышел из этого диапазона, то можно сказать, что произошла разладка.



\section{Вычислительный эксперимент}

   \par Продемонстрируем ход алгоритма, описанного выше, на практике и явно покажем, как осуществляется процесс поиска точек разладки временного ряда.

   \par Эксперимент проводится над выборкой из $N=150$ элементов. В качестве тестовой выборки берётся искусственно зашумленный ряд синусов. 
   
   \begin{figure}[h]
   \centering
    \includegraphics[width=5.5cm]{sample_1 (2).jpg}
    \includegraphics[width=5.5cm]{sample_2 (2).jpg}
    \caption{тестовая выборка}
    \label{fig:g}
   \end{figure}
   
   \par Таким образом явное выражение $n$-ого члена временного ряда выражается по формуле:
   \begin{equation} X_n = sin(n) + \xi_n\end{equation}
   \par --- где $\xi_n$ случайная величина, распределение которой задаётся следующим образом:
   \[
   \xi_n \sim \left\{
     \begin{array}{@{}l@{\thinspace}l}
       \mathcal{N}(0,1)  &: n < 50 \text{ or } n > 100\\
       \mathcal{U}(0,1) &: 50 \leq n \leq 100 \\
     \end{array} \right
   \]
   \par С помощью модели Neural SDE, реализованной библиотекой torchsde, строится прогноз продолжения полученного ряда.

   \par По приращениям элементов прогноза получим матрицу фазовых траекторий процесса и сравнивая её с целевой матрицей Ганкеля на фиксированном окне запустим процесс обучения модели. Цель: подобрать параметры drift и diffusion такие, чтобы отклонение от target-a было минимальным.

   \begin{figure}[h]
    \centering
\includegraphics[width=10cm]{loss (2).jpg}
    \caption{процесс обучения}
    \label{fig:galaxy}
    \end{figure}

    \par По итогу обучения получаем следующий результат работы алгоритма на исходном временном ряду.


   \centering
   \includegraphics[width=10cm]{result (2).jpg}
   \label{fig:galaxy}


   \par Доверительные интервалы пускай и не точно, но позволяют оценить примерное число точек разладки и место их наибольшего сосредоточения: как раз тот самый отрезок ряда от 50-го до 100-го элементов, который был ранее зашумлен нормальным шумом, а не равномерным в отличие от остальных.

   \par Итоговое сравнение с другими методами поисками точек разладки показывает то, что NeuralSDE выигрывает в плане точности, но уступает в плане скорости.

   \begin{equation}
 \begin{bmatrix}
   Model & Speed(sec) & Error  \\
   Neural SDE & 47.2 & 0.044 \\
   SSA  & 6.48  & 0.12  \\
   CuSum & 3.56 & 0.08 \\
   CuSumSqr & 20.03 & 0.061
 \end{bmatrix}
\end{equation}
    
\section{Заключение}

   \par Результатом проведенной работы является обоснование корректности нового способа поиска точек разладки на практике. Плюсы и минусы метода ясны, и зная их, исследователь способен использовать его как новый и полезный инструмент.



\bibliographystyle{unsrtnat}
\bibliography{references}

[1] “Neural Ordinary Differential Equations Ricky” T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud \\

[2] “Neural SDE: Stabilizing Neural ODE Networks with Stochastic Noise” Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, Cho-Jui Hsieh  \\

[3] “Riemannian Neural SDE: Learning Stochastic Representations on Manifolds” Sung Woo Park , Hyomin Kim , Kyungjae Lee , Junseok Kwon \\

[4] “Riemannian Diffusion Models” Chin-Wei Huang, Milad Aghajohari, Avishek Joey Bose, Prakash Panangaden, Aaron Courville \\

[5] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In Advances in Neural Information Processing Systems, pages 6572–6583, 2018. \\

[6] F.Yu. Yaushev, R. V. Isachenko, V. V. Strijov. Concordant models for latent space projections in forecasting, 2020. \\

[7] Neural Controlled Differential Equations for Irregular Time Series
Patrick Kidger, James Morrill, James Foster, Terry Lyons \\

[8] Neural Ordinary Differential Equations
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud \\

[9] Discriminative models for multi-instance problems with tree-structure
Tomas Pevny, Petr Somol \\

[10] Training Discriminative Models to Evaluate Generative Ones
Timothée Lesort, Andrei Stoain, Jean-François Goudou, David Filliat \\

[11] Discriminative Neural Topic Models
Gaurav Pandey, Ambedkar Dukkipati \\

[12] Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models
Sam Bond-Taylor, Adam Leach, Yang Long, Chris G. Willcocks \\

\end{document}